{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11997169,"sourceType":"datasetVersion","datasetId":7372625}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3ff7f06d","cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nimport sys\nsys.path.append('/kaggle/input/eye-tracking-dataset')\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport spacy\nfrom transformers import AutoTokenizer, RobertaModel, BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom utils import *\nfrom word_fixations import *\nfrom word_properties import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:44:04.095609Z","iopub.execute_input":"2025-05-30T11:44:04.095944Z","iopub.status.idle":"2025-05-30T11:44:04.186918Z","shell.execute_reply.started":"2025-05-30T11:44:04.095918Z","shell.execute_reply":"2025-05-30T11:44:04.186240Z"}},"outputs":[{"name":"stdout","text":"The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n","output_type":"stream"}],"execution_count":8},{"id":"aede1d5a-44c3-4112-a37a-f3e70cd8e666","cell_type":"code","source":"!pip install wordfreq\n!pip install pyphen\n!pip install surprisal","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:43:51.406979Z","iopub.execute_input":"2025-05-30T11:43:51.407263Z","iopub.status.idle":"2025-05-30T11:44:00.356443Z","shell.execute_reply.started":"2025-05-30T11:43:51.407232Z","shell.execute_reply":"2025-05-30T11:44:00.355732Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wordfreq in /usr/local/lib/python3.11/dist-packages (3.1.1)\nRequirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (6.3.1)\nRequirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (3.5.0)\nRequirement already satisfied: locate<2.0.0,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (1.1.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (1.1.0)\nRequirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (2024.11.6)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes>=3.0->wordfreq) (1.3.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.2.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from marisa-trie>=1.1.0->language-data>=1.2->langcodes>=3.0->wordfreq) (75.1.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pyphen in /usr/local/lib/python3.11/dist-packages (0.17.2)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: surprisal in /usr/local/lib/python3.11/dist-packages (0.1.6)\nRequirement already satisfied: matplotlib<4.0.0,>=3.5.2 in /usr/local/lib/python3.11/dist-packages (from surprisal) (3.7.5)\nRequirement already satisfied: numpy<2.0.0,>=1.23.1 in /usr/local/lib/python3.11/dist-packages (from surprisal) (1.26.4)\nRequirement already satisfied: plotext<6.0.0,>=5.0.2 in /usr/local/lib/python3.11/dist-packages (from surprisal) (5.3.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.5.2->surprisal) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.5.2->surprisal) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.5.2->surprisal) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.5.2->surprisal) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.5.2->surprisal) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.5.2->surprisal) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.5.2->surprisal) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.5.2->surprisal) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.1->surprisal) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.1->surprisal) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.1->surprisal) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.1->surprisal) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.1->surprisal) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.23.1->surprisal) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.5.2->surprisal) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.23.1->surprisal) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.23.1->surprisal) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.23.1->surprisal) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.23.1->surprisal) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.23.1->surprisal) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"id":"bb94e82a-6806-4228-9c9b-df525248db5a","cell_type":"code","source":"words_dict = get_merged_words_dict_from_csv(csv_path='/kaggle/input/eye-tracking-dataset/word_sentence_fixations/words_dict_romanian_merged.csv', properties_dir='/kaggle/input/eye-tracking-dataset/properties/properties_romanian_009')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:44:08.682660Z","iopub.execute_input":"2025-05-30T11:44:08.682963Z","iopub.status.idle":"2025-05-30T11:44:09.123017Z","shell.execute_reply.started":"2025-05-30T11:44:08.682939Z","shell.execute_reply":"2025-05-30T11:44:09.122417Z"}},"outputs":[],"execution_count":9},{"id":"24740ca1","cell_type":"code","source":"# Create a dictionary excluding non-page stimuli\nwords_dict_reading = {}\nfor stimulus_key in words_dict:\n\tif 'page' in stimulus_key:\n\t\twords_dict_reading[stimulus_key] = words_dict[stimulus_key]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:44:10.946369Z","iopub.execute_input":"2025-05-30T11:44:10.946611Z","iopub.status.idle":"2025-05-30T11:44:11.021714Z","shell.execute_reply.started":"2025-05-30T11:44:10.946594Z","shell.execute_reply":"2025-05-30T11:44:11.021146Z"}},"outputs":[],"execution_count":10},{"id":"d2844249-198d-402d-89e7-a87504157b3b","cell_type":"code","source":"import math, numpy as np, pandas as pd, torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (BertModel, BertForTokenClassification, AutoTokenizer,\n                          get_cosine_schedule_with_warmup)\nfrom sklearn.metrics import r2_score\nfrom scipy.stats import pearsonr, spearmanr\n\n# --------------------------------\n# Data Preparation\n# --------------------------------\n# Get data from dict\nwords = [words_dict_reading[stimulus_key][word_idx]['word']\n         for stimulus_key in words_dict_reading\n         for word_idx in words_dict_reading[stimulus_key]]\nsentences = [words_dict_reading[stimulus_key][word_idx]['sentence']\n             for stimulus_key in words_dict_reading\n             for word_idx in words_dict_reading[stimulus_key]]\nsentence_ids = [words_dict_reading[stimulus_key][word_idx]['sentence_id']\n                for stimulus_key in words_dict_reading\n                for word_idx in words_dict_reading[stimulus_key]]\ntrt = [words_dict_reading[stimulus_key][word_idx]['average_TRT']\n       for stimulus_key in words_dict_reading\n       for word_idx in words_dict_reading[stimulus_key]]\n\ndata_sentences = {}\nfor i in range(len(words)):\n    sid = sentence_ids[i]\n    if sid not in data_sentences:\n        data_sentences[sid] = {'sentence': sentences[i], 'words': [], 'trt': []}\n    data_sentences[sid]['words'].append(words[i])\n    data_sentences[sid]['trt'].append(trt[i])\n\n# Turn into dataframe\ndata_sentences = pd.DataFrame(data_sentences).T\n\n# Split\ntrain_df = data_sentences.sample(frac=0.8, random_state=42)\ntmp_df   = data_sentences.drop(train_df.index)\nval_df   = tmp_df.sample(frac=0.5, random_state=42)\ntest_df  = tmp_df.drop(val_df.index)\n\n# Standardize trt\ntrain_trt = np.concatenate(train_df[\"trt\"].values)\ntrt_mean, trt_std = train_trt.mean(), train_trt.std()\nfor df in [train_df, val_df, test_df]:\n    df[\"trt\"] = df[\"trt\"].apply(lambda lst: [(t - trt_mean) / trt_std for t in lst])\n\n\n# --------------------------------\n# Dataset\n# --------------------------------\nclass ReadingTimeDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, tokenizer, max_len: int = 128):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self): \n        return len(self.df)\n\n    def __getitem__(self, idx):\n        words   = self.df.loc[idx, \"sentence\"].split()\n        targets = self.df.loc[idx, \"trt\"]\n\n        enc = self.tokenizer(words, is_split_into_words=True, truncation=True,\n                             padding=\"max_length\", max_length=self.max_len,\n                             return_attention_mask=True)\n\n        wp_labels = []\n        cur = None\n        for w_id in enc.word_ids():\n            if w_id is None:\n                wp_labels.append(0.0)\n            else:\n                if cur != w_id: cur = w_id\n                wp_labels.append(targets[cur])\n\n        return {\n            \"ids\":     torch.tensor(enc[\"input_ids\"], dtype=torch.long),\n            \"mask\":    torch.tensor(enc[\"attention_mask\"], dtype=torch.long),\n            \"targets\": torch.tensor(wp_labels, dtype=torch.float32)\n        }\n\n\n# --------------------------------\n# Loss\n# --------------------------------\ndef masked_mse_loss(preds, targets, word_ids):\n    mask = torch.tensor([0 if w_id is None else 1 for w_id in word_ids],\n                        dtype=torch.float32, device=preds.device)\n    loss = (preds - targets) ** 2\n    loss = loss * mask\n    return loss.sum() / mask.sum()\n\ndef aggregate_token_preds_to_words(token_preds, word_ids):\n    word_to_token_preds = {}\n    for i, w_id in enumerate(word_ids):\n        if w_id is None:\n            continue\n        word_to_token_preds.setdefault(w_id, []).append(token_preds[i])\n    return torch.stack([torch.stack(v).mean() for k, v in sorted(word_to_token_preds.items())])\n\n\n# --------------------------------\n# Models\n# --------------------------------\nclass BertRegression1(nn.Module):\n    def __init__(self, model_name=\"dumitrescustefan/bert-base-romanian-uncased-v1\", dropout=0.3):\n        super().__init__()\n        self.model = BertForTokenClassification.from_pretrained(model_name, num_labels=1, hidden_dropout_prob=dropout)\n\n    def forward(self, ids, mask):\n        out = self.model(input_ids=ids, attention_mask=mask, return_dict=True)\n        return out.logits.squeeze(-1)\n\n\nclass BertRegressionHead(nn.Module):\n    def __init__(self, hidden_size=768, dropout=0.1):\n        super().__init__()\n        self.regressor = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, 1)\n        )\n\n    def forward(self, x, mask):\n        logits = self.regressor(x).squeeze(-1)\n        return logits * mask\n\nclass BertRegression2(nn.Module):\n    def __init__(self, bert_model_name=\"dumitrescustefan/bert-base-romanian-uncased-v1\", dropout=0.3):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        self.head = BertRegressionHead(self.bert.config.hidden_size, dropout)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        return self.head(outputs.last_hidden_state, attention_mask)\n\n\n# --------------------------------\n# Train, evaluate and freezing utils\n# --------------------------------\ndef freeze_bert_layers(model, num_layers_to_unfreeze=0):\n    \"\"\"\n    Freeze all BERT layers except the last `num_layers_to_unfreeze.\n    \"\"\"\n    if hasattr(model, 'model'):\n        bert = model.model.bert\n    elif hasattr(model, 'bert'):\n        bert = model.bert\n    else:\n        raise AttributeError(\"Model has no BERT encoder attribute\")\n\n    bert_layers = list(bert.encoder.layer)\n    total_layers = len(bert_layers)\n\n    for i, layer in enumerate(bert_layers):\n        requires_grad = i >= (total_layers - num_layers_to_unfreeze)\n        for param in layer.parameters():\n            param.requires_grad = requires_grad\n\n    # Embeddings\n    for param in bert.embeddings.parameters():\n        param.requires_grad = (num_layers_to_unfreeze > 0)\n\n\ndef freeze_all_bert_layers(model):\n    if hasattr(model, 'model'):\n        bert = model.model.bert\n    elif hasattr(model, 'bert'):\n        bert = model.bert\n    else:\n        raise AttributeError(\"Model has no BERT encoder attribute\")\n    for param in bert.parameters():\n        param.requires_grad = False\n\n\ndef unfreeze_bert_layers(model, num_layers_to_unfreeze):\n    if hasattr(model, 'model'):\n        bert = model.model.bert\n    elif hasattr(model, 'bert'):\n        bert = model.bert\n    else:\n        raise AttributeError(\"Model has no BERT encoder attribute\")\n\n    bert_layers = list(bert.encoder.layer)\n    total_layers = len(bert_layers)\n\n    for i, layer in enumerate(bert_layers):\n        requires_grad = i >= (total_layers - num_layers_to_unfreeze)\n        for param in layer.parameters():\n            param.requires_grad = requires_grad\n\n    for param in bert.embeddings.parameters():\n        param.requires_grad = (num_layers_to_unfreeze > 0)\n\n\ndef train_word_level(\n    model, train_loader, val_loader, tokenizer, device, epochs, model_path,\n    initial_unfrozen_layers=0, base_lr=1e-4, weight_decay=1e-4\n):\n    model.to(device)\n\n    # Freeze all layers except the last `initial_unfrozen_layers`\n    freeze_bert_layers(model, initial_unfrozen_layers)\n\n    # Create optimizer and scheduler once at the start\n    optimizer = torch.optim.AdamW(\n        filter(lambda p: p.requires_grad, model.parameters()),\n        lr=base_lr,\n        weight_decay=weight_decay\n    )\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=10,\n        num_training_steps=len(train_loader) * epochs\n    )\n\n    best_val_loss = float('inf')\n\n    for epoch in range(epochs):\n        model.train()\n        total_train_loss = 0.0\n        total_train_tokens = 0\n\n        for idx, batch in enumerate(train_loader):\n            ids, mask, y = [b.to(device) for b in batch.values()]\n            out = model(ids, mask)\n            optimizer.zero_grad()\n\n            batch_loss = 0.0\n            batch_tokens = 0\n\n            for b in range(ids.size(0)):\n                row_idx = idx * train_loader.batch_size + b\n                words = train_loader.dataset.df.loc[row_idx, \"sentence\"].split()\n                enc = tokenizer(words, is_split_into_words=True, truncation=True, padding=\"max_length\",\n                                max_length=train_loader.dataset.max_len)\n                word_ids = enc.word_ids()\n\n                token_preds = out[b]\n                token_targets = y[b]\n\n                mask_tensor = torch.tensor([0 if wid is None else 1 for wid in word_ids],\n                                           dtype=torch.float32, device=token_preds.device)\n                loss = ((token_preds - token_targets) ** 2) * mask_tensor\n                batch_loss += loss.sum()\n                batch_tokens += mask_tensor.sum().item()\n\n            batch_loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n\n            total_train_loss += batch_loss.item()\n            total_train_tokens += batch_tokens\n\n        train_loss = total_train_loss / total_train_tokens\n        val_loss, r2, pearson, spearman, accuracy = evaluate_word_level(model, val_loader, tokenizer, device)\n\n        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n              f\"R2: {r2:.4f}, Pearson: {pearson:.4f}, Spearman: {spearman:.4f}, Accuracy: {accuracy:.2f}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), model_path)\n            print(\"→ Saved new best model.\")\n\n\ndef gradual_unfreeze_and_train(\n    model, train_dl, val_dl, tokenizer, device, model_path,\n    total_epochs=20, unfreeze_every=5, max_unfrozen_layers=None,\n    base_lr=1e-4, weight_decay=1e-4\n):\n    \"\"\"\n    Gradually unfreeze BERT layers every unfreeze_every epochs and train.\n    \"\"\"\n\n    # Determine max layers to unfreeze\n    if max_unfrozen_layers is None:\n        if hasattr(model, 'model'):\n            max_unfrozen_layers = len(list(model.model.bert.encoder.layer))\n        elif hasattr(model, 'bert'):\n            max_unfrozen_layers = len(list(model.bert.encoder.layer))\n        else:\n            raise AttributeError(\"Model has no BERT encoder attribute\")\n\n    epochs_per_phase = unfreeze_every\n    phases = total_epochs // epochs_per_phase\n\n    unfrozen_layers = 2\n\n    for phase in range(phases):\n        print(f\"\\n=== Training Phase {phase+1}/{phases} with {unfrozen_layers}/{max_unfrozen_layers} BERT layers unfrozen ===\\n\")\n        freeze_bert_layers(model, unfrozen_layers)\n\n        train_word_level(\n            model=model,\n            train_loader=train_dl,\n            val_loader=val_dl,\n            tokenizer=tokenizer,\n            device=device,\n            epochs=epochs_per_phase,\n            model_path=model_path,\n            initial_unfrozen_layers=unfrozen_layers,\n            base_lr=base_lr,\n            weight_decay=weight_decay,\n        )\n\n        unfrozen_layers = min(unfrozen_layers + 2, max_unfrozen_layers)\n\n\ndef train_head_then_full_model(\n    model, train_dl, val_dl, tokenizer, device, model_path_head_only, model_path_full,\n    head_only_epochs=5, total_epochs=20, unfreeze_every=5,\n    base_lr=1e-4, weight_decay=1e-4\n):\n    # Phase 1: Freeze BERT completely and train only the head\n    print(\"=== Phase 1: Training head only (BERT frozen) ===\")\n    freeze_all_bert_layers(model)\n    \n    train_word_level(\n        model=model,\n        train_loader=train_dl,\n        val_loader=val_dl,\n        tokenizer=tokenizer,\n        device=device,\n        epochs=head_only_epochs,\n        model_path=model_path_head_only,\n        initial_unfrozen_layers=1,  # BERT layers frozen\n        base_lr=base_lr,\n        weight_decay=weight_decay\n    )\n    \n    model.load_state_dict(torch.load(model_path_head_only))\n    \n    # Phase 2: Gradually unfreeze and train full model\n    print(\"\\n=== Phase 2: Gradual unfreeze and full model training ===\")\n    gradual_unfreeze_and_train(\n        model,\n        train_dl,\n        val_dl,\n        tokenizer,\n        device,\n        model_path=model_path_full,\n        total_epochs=total_epochs - head_only_epochs,\n        unfreeze_every=unfreeze_every,\n        base_lr=base_lr,\n        weight_decay=weight_decay\n    )\n\n\ndef evaluate_word_level(model, val_loader, tokenizer, device):\n    model.eval()\n    model.to(device)\n    losses = []\n    preds, golds = [], []\n\n    with torch.no_grad():\n        for idx, batch in enumerate(val_loader):\n            ids, mask, y = [b.to(device) for b in batch.values()]\n            out = model(ids, mask)\n\n            for b in range(ids.size(0)):\n                row_idx = idx * val_loader.batch_size + b\n                words = val_loader.dataset.df.loc[row_idx, \"sentence\"].split()\n                enc = tokenizer(words, is_split_into_words=True, truncation=True, padding=\"max_length\",\n                                max_length=val_loader.dataset.max_len)\n                word_ids = enc.word_ids()\n\n                token_preds = out[b]\n                token_targets = y[b]\n                loss = masked_mse_loss(token_preds, token_targets, word_ids)\n                losses.append(loss.item())\n\n                word_preds = aggregate_token_preds_to_words(token_preds.cpu(), word_ids)\n                word_targets = aggregate_token_preds_to_words(token_targets.cpu(), word_ids)\n\n                preds.extend(word_preds.tolist())\n                golds.extend(word_targets.tolist())\n\n    r2 = r2_score(golds, preds)\n    pearson = pearsonr(golds, preds)[0]\n    spearman = spearmanr(golds, preds)[0]\n    \n    golds_np = np.array(golds) * trt_std + trt_mean\n    preds_np = np.array(preds) * trt_std + trt_mean\n    golds_100 = (golds_np - np.min(golds_np)) / (np.max(golds_np) - np.min(golds_np)) * 100\n    preds_100 = (preds_np - np.min(preds_np)) / (np.max(preds_np) - np.min(preds_np)) * 100\n    mae = np.abs(golds_100 - preds_100).mean()\n    accuracy = 100 - mae\n\n    return np.mean(losses), r2, pearson, spearman, accuracy\n\n\n# --------------------------------\n# Execution\n# --------------------------------\nMAX_LEN = 256\nBATCH = 8\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-uncased-v1\")\n\ntrain_dl = DataLoader(ReadingTimeDataset(train_df, tokenizer, MAX_LEN), batch_size=BATCH, shuffle=True)\nval_dl   = DataLoader(ReadingTimeDataset(val_df, tokenizer, MAX_LEN), batch_size=BATCH)\ntest_dl  = DataLoader(ReadingTimeDataset(test_df, tokenizer, MAX_LEN), batch_size=BATCH)\n\n# === Model 1 ===\nmodel1 = BertRegression1(\"dumitrescustefan/bert-base-romanian-uncased-v1\", dropout=0.3)\n\ngradual_unfreeze_and_train(\n    model1,\n    train_dl,\n    val_dl,\n    tokenizer,\n    device,\n    model_path=\"bert_token_cls_best_1.pt\",\n    total_epochs=30,\n    unfreeze_every=5,\n    base_lr=1e-4,\n    weight_decay=1e-4\n)\n\nmodel1.load_state_dict(torch.load(\"bert_token_cls_best_1.pt\"))\ntest_loss, r2, pearson, spearman, accuracy = evaluate_word_level(model1, test_dl, tokenizer, device)\nprint(f\"\\nModel 1 Test → Loss: {test_loss:.4f}, R2: {r2:.4f}, Pearson: {pearson:.4f}, Spearman: {spearman:.4f}, Accuracy: {accuracy:.2f}\\n\")\n\n\n# === Model 2 ===\nmodel2 = BertRegression2(\"dumitrescustefan/bert-base-romanian-uncased-v1\", dropout=0.3)\n\ntrain_head_then_full_model(\n    model2,\n    train_dl,\n    val_dl,\n    tokenizer,\n    device,\n    model_path_head_only=\"bert_regression2_head_only.pt\",\n    model_path_full=\"bert_regression2_best.pt\",\n    head_only_epochs=5,\n    total_epochs=35,\n    unfreeze_every=5,\n    base_lr=1e-4,\n    weight_decay=1e-4\n)\n\nmodel2.load_state_dict(torch.load(\"bert_regression2_best.pt\"))\ntest_loss2, r2_2, pearson_2, spearman_2, accuracy_2 = evaluate_word_level(model2, test_dl, tokenizer, device)\nprint(f\"\\nModel 2 Test → Loss: {test_loss2:.4f}, R2: {r2_2:.4f}, Pearson: {pearson_2:.4f}, Spearman: {spearman_2:.4f}, Accuracy: {accuracy_2:.2f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T14:53:42.174823Z","iopub.execute_input":"2025-05-30T14:53:42.175572Z","iopub.status.idle":"2025-05-30T15:03:31.964183Z","shell.execute_reply.started":"2025-05-30T14:53:42.175550Z","shell.execute_reply":"2025-05-30T15:03:31.963491Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at dumitrescustefan/bert-base-romanian-uncased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n=== Training Phase 1/6 with 2/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 1.0427, Val Loss: 1.9067, R2: 0.1857, Pearson: 0.4820, Spearman: 0.5346, Accuracy: 63.93\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.7994, Val Loss: 1.6463, R2: 0.2958, Pearson: 0.5499, Spearman: 0.6038, Accuracy: 64.66\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.7492, Val Loss: 1.6892, R2: 0.2612, Pearson: 0.5575, Spearman: 0.6125, Accuracy: 63.37\nEpoch 4/5 - Train Loss: 0.6083, Val Loss: 1.6613, R2: 0.2862, Pearson: 0.5614, Spearman: 0.6171, Accuracy: 64.09\nEpoch 5/5 - Train Loss: 0.5780, Val Loss: 1.6830, R2: 0.2722, Pearson: 0.5607, Spearman: 0.6168, Accuracy: 65.00\n\n=== Training Phase 2/6 with 4/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.8105, Val Loss: 2.0158, R2: 0.0803, Pearson: 0.5312, Spearman: 0.6043, Accuracy: 66.49\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.5997, Val Loss: 1.7357, R2: 0.2649, Pearson: 0.5775, Spearman: 0.6352, Accuracy: 65.65\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.5297, Val Loss: 1.5142, R2: 0.3065, Pearson: 0.5739, Spearman: 0.6295, Accuracy: 74.66\n→ Saved new best model.\nEpoch 4/5 - Train Loss: 0.3774, Val Loss: 1.5966, R2: 0.2805, Pearson: 0.5708, Spearman: 0.6281, Accuracy: 72.27\nEpoch 5/5 - Train Loss: 0.3675, Val Loss: 1.5399, R2: 0.3057, Pearson: 0.5711, Spearman: 0.6290, Accuracy: 73.10\n\n=== Training Phase 3/6 with 6/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.4433, Val Loss: 1.4189, R2: 0.3255, Pearson: 0.5812, Spearman: 0.6209, Accuracy: 77.83\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.3453, Val Loss: 1.3874, R2: 0.3391, Pearson: 0.5923, Spearman: 0.6310, Accuracy: 76.43\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.3775, Val Loss: 1.3232, R2: 0.3640, Pearson: 0.6183, Spearman: 0.6401, Accuracy: 82.37\n→ Saved new best model.\nEpoch 4/5 - Train Loss: 0.2696, Val Loss: 1.3129, R2: 0.3509, Pearson: 0.6112, Spearman: 0.6376, Accuracy: 86.07\n→ Saved new best model.\nEpoch 5/5 - Train Loss: 0.2242, Val Loss: 1.2992, R2: 0.3573, Pearson: 0.6149, Spearman: 0.6396, Accuracy: 86.63\n→ Saved new best model.\n\n=== Training Phase 4/6 with 8/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.2572, Val Loss: 1.4065, R2: 0.3224, Pearson: 0.6212, Spearman: 0.6478, Accuracy: 85.31\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.2725, Val Loss: 1.3158, R2: 0.3627, Pearson: 0.6141, Spearman: 0.6408, Accuracy: 83.54\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.2702, Val Loss: 1.3691, R2: 0.3606, Pearson: 0.6269, Spearman: 0.6583, Accuracy: 86.45\nEpoch 4/5 - Train Loss: 0.1868, Val Loss: 1.4812, R2: 0.3343, Pearson: 0.5933, Spearman: 0.6464, Accuracy: 86.26\nEpoch 5/5 - Train Loss: 0.1921, Val Loss: 1.4875, R2: 0.3318, Pearson: 0.5951, Spearman: 0.6473, Accuracy: 85.43\n\n=== Training Phase 5/6 with 10/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.1973, Val Loss: 1.4350, R2: 0.3086, Pearson: 0.6018, Spearman: 0.6410, Accuracy: 91.17\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.3098, Val Loss: 1.3653, R2: 0.3572, Pearson: 0.6056, Spearman: 0.6409, Accuracy: 83.09\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.2052, Val Loss: 1.4084, R2: 0.3590, Pearson: 0.6108, Spearman: 0.6441, Accuracy: 86.29\nEpoch 4/5 - Train Loss: 0.1778, Val Loss: 1.4841, R2: 0.3157, Pearson: 0.6056, Spearman: 0.6430, Accuracy: 87.36\nEpoch 5/5 - Train Loss: 0.1304, Val Loss: 1.4320, R2: 0.3471, Pearson: 0.6075, Spearman: 0.6441, Accuracy: 87.46\n\n=== Training Phase 6/6 with 12/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.1509, Val Loss: 1.6271, R2: 0.1908, Pearson: 0.5797, Spearman: 0.6157, Accuracy: 91.13\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.1685, Val Loss: 1.5352, R2: 0.3474, Pearson: 0.6177, Spearman: 0.6513, Accuracy: 81.08\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.2157, Val Loss: 1.4814, R2: 0.3404, Pearson: 0.6056, Spearman: 0.6450, Accuracy: 86.39\n→ Saved new best model.\nEpoch 4/5 - Train Loss: 0.1325, Val Loss: 1.3460, R2: 0.3750, Pearson: 0.6256, Spearman: 0.6490, Accuracy: 86.19\n→ Saved new best model.\nEpoch 5/5 - Train Loss: 0.1075, Val Loss: 1.3685, R2: 0.3684, Pearson: 0.6233, Spearman: 0.6496, Accuracy: 86.00\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3984016617.py:428: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model1.load_state_dict(torch.load(\"bert_token_cls_best_1.pt\"))\n","output_type":"stream"},{"name":"stdout","text":"\nModel 1 Test → Loss: 1.0952, R2: 0.4075, Pearson: 0.6516, Spearman: 0.6847, Accuracy: 85.10\n\n=== Phase 1: Training head only (BERT frozen) ===\nEpoch 1/10 - Train Loss: 1.0162, Val Loss: 1.8315, R2: 0.1984, Pearson: 0.5045, Spearman: 0.5389, Accuracy: 64.49\n→ Saved new best model.\nEpoch 2/10 - Train Loss: 0.7550, Val Loss: 1.4811, R2: 0.2923, Pearson: 0.5535, Spearman: 0.5904, Accuracy: 67.35\n→ Saved new best model.\nEpoch 3/10 - Train Loss: 0.6203, Val Loss: 1.5578, R2: 0.2439, Pearson: 0.5551, Spearman: 0.5776, Accuracy: 74.78\nEpoch 4/10 - Train Loss: 0.4477, Val Loss: 1.5302, R2: 0.2521, Pearson: 0.5823, Spearman: 0.6182, Accuracy: 78.39\nEpoch 5/10 - Train Loss: 0.3636, Val Loss: 1.6129, R2: 0.2969, Pearson: 0.5604, Spearman: 0.5963, Accuracy: 77.52\nEpoch 6/10 - Train Loss: 0.3063, Val Loss: 1.5574, R2: 0.2954, Pearson: 0.5635, Spearman: 0.6097, Accuracy: 79.25\nEpoch 7/10 - Train Loss: 0.2602, Val Loss: 1.5680, R2: 0.2820, Pearson: 0.5582, Spearman: 0.6050, Accuracy: 80.06\nEpoch 8/10 - Train Loss: 0.2594, Val Loss: 1.5527, R2: 0.2904, Pearson: 0.5591, Spearman: 0.6065, Accuracy: 80.76\nEpoch 9/10 - Train Loss: 0.2566, Val Loss: 1.5421, R2: 0.2931, Pearson: 0.5635, Spearman: 0.6076, Accuracy: 80.14\nEpoch 10/10 - Train Loss: 0.2135, Val Loss: 1.5428, R2: 0.2928, Pearson: 0.5633, Spearman: 0.6075, Accuracy: 80.12\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3984016617.py:338: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path_head_only))\n","output_type":"stream"},{"name":"stdout","text":"\n=== Phase 2: Gradual unfreeze and full model training ===\n\n=== Training Phase 1/6 with 2/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.6132, Val Loss: 1.6034, R2: 0.2386, Pearson: 0.5620, Spearman: 0.5948, Accuracy: 78.00\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.4294, Val Loss: 1.5281, R2: 0.3090, Pearson: 0.5623, Spearman: 0.6034, Accuracy: 75.26\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.4214, Val Loss: 1.6039, R2: 0.2548, Pearson: 0.5426, Spearman: 0.5885, Accuracy: 81.53\nEpoch 4/5 - Train Loss: 0.2818, Val Loss: 1.5190, R2: 0.2869, Pearson: 0.5584, Spearman: 0.5994, Accuracy: 79.77\n→ Saved new best model.\nEpoch 5/5 - Train Loss: 0.2880, Val Loss: 1.5323, R2: 0.2867, Pearson: 0.5588, Spearman: 0.6002, Accuracy: 79.79\n\n=== Training Phase 2/6 with 4/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.3102, Val Loss: 1.9042, R2: 0.1086, Pearson: 0.5436, Spearman: 0.5808, Accuracy: 83.11\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.2741, Val Loss: 1.4517, R2: 0.2621, Pearson: 0.5824, Spearman: 0.6263, Accuracy: 79.39\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.2821, Val Loss: 1.4594, R2: 0.2400, Pearson: 0.5829, Spearman: 0.6236, Accuracy: 84.04\nEpoch 4/5 - Train Loss: 0.1571, Val Loss: 1.5061, R2: 0.3241, Pearson: 0.5876, Spearman: 0.6225, Accuracy: 83.09\nEpoch 5/5 - Train Loss: 0.1271, Val Loss: 1.4463, R2: 0.3454, Pearson: 0.5911, Spearman: 0.6256, Accuracy: 83.20\n→ Saved new best model.\n\n=== Training Phase 3/6 with 6/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.1582, Val Loss: 1.7200, R2: 0.2559, Pearson: 0.5370, Spearman: 0.6110, Accuracy: 84.54\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.2939, Val Loss: 1.4225, R2: 0.2852, Pearson: 0.5462, Spearman: 0.6009, Accuracy: 87.57\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.1928, Val Loss: 1.4263, R2: 0.3196, Pearson: 0.5731, Spearman: 0.6151, Accuracy: 83.67\nEpoch 4/5 - Train Loss: 0.1983, Val Loss: 1.4034, R2: 0.3238, Pearson: 0.5725, Spearman: 0.6206, Accuracy: 84.74\n→ Saved new best model.\nEpoch 5/5 - Train Loss: 0.1008, Val Loss: 1.3946, R2: 0.3310, Pearson: 0.5806, Spearman: 0.6250, Accuracy: 84.37\n→ Saved new best model.\n\n=== Training Phase 4/6 with 8/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.1161, Val Loss: 1.7052, R2: 0.2251, Pearson: 0.5245, Spearman: 0.6135, Accuracy: 91.29\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.2062, Val Loss: 1.4631, R2: 0.3148, Pearson: 0.5667, Spearman: 0.6238, Accuracy: 83.97\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.2035, Val Loss: 1.4737, R2: 0.3487, Pearson: 0.5926, Spearman: 0.6519, Accuracy: 85.74\nEpoch 4/5 - Train Loss: 0.1101, Val Loss: 1.4841, R2: 0.3295, Pearson: 0.5792, Spearman: 0.6445, Accuracy: 86.23\nEpoch 5/5 - Train Loss: 0.0840, Val Loss: 1.4863, R2: 0.3320, Pearson: 0.5802, Spearman: 0.6448, Accuracy: 85.84\n\n=== Training Phase 5/6 with 10/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.1396, Val Loss: 1.5996, R2: 0.2820, Pearson: 0.5577, Spearman: 0.6309, Accuracy: 76.34\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.2090, Val Loss: 1.7257, R2: 0.2056, Pearson: 0.5566, Spearman: 0.6282, Accuracy: 81.85\nEpoch 3/5 - Train Loss: 0.1188, Val Loss: 1.7874, R2: 0.2763, Pearson: 0.5432, Spearman: 0.6365, Accuracy: 89.42\nEpoch 4/5 - Train Loss: 0.0926, Val Loss: 1.6785, R2: 0.2967, Pearson: 0.5585, Spearman: 0.6388, Accuracy: 87.69\nEpoch 5/5 - Train Loss: 0.0802, Val Loss: 1.6919, R2: 0.2963, Pearson: 0.5535, Spearman: 0.6384, Accuracy: 87.89\n\n=== Training Phase 6/6 with 12/12 BERT layers unfrozen ===\n\nEpoch 1/5 - Train Loss: 0.0892, Val Loss: 1.8154, R2: 0.2836, Pearson: 0.5523, Spearman: 0.6319, Accuracy: 89.33\n→ Saved new best model.\nEpoch 2/5 - Train Loss: 0.1309, Val Loss: 1.6480, R2: 0.3054, Pearson: 0.5692, Spearman: 0.6432, Accuracy: 90.04\n→ Saved new best model.\nEpoch 3/5 - Train Loss: 0.1134, Val Loss: 1.3217, R2: 0.3678, Pearson: 0.6163, Spearman: 0.6520, Accuracy: 84.22\n→ Saved new best model.\nEpoch 4/5 - Train Loss: 0.1024, Val Loss: 1.4429, R2: 0.3447, Pearson: 0.5929, Spearman: 0.6486, Accuracy: 83.25\nEpoch 5/5 - Train Loss: 0.0720, Val Loss: 1.4280, R2: 0.3512, Pearson: 0.5970, Spearman: 0.6504, Accuracy: 84.10\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3984016617.py:451: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model2.load_state_dict(torch.load(\"bert_regression2_best.pt\"))\n","output_type":"stream"},{"name":"stdout","text":"\nModel 2 Test → Loss: 1.1241, R2: 0.3956, Pearson: 0.6524, Spearman: 0.6860, Accuracy: 83.30\n\n","output_type":"stream"}],"execution_count":50}]}